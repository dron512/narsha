#결정트리

'''
    한빛 마켓에서는 신상품으로 캔 와인을 판매하려 합니다. 주류는 온라인 판매가 안돼서 온라인
    예약 후 오프라인 매장에서 구매를 유도할 계획입니다.

    아무래도 병이 아닌 캔이라 걱정인데 마켓팅 팀은 특수 캔으로 맛과 향이 유지되도록 제작했다며 젊은 층에
    인기가 있을거라 자신합니다. 그런데 입고된 와인을 보니 급하게 제작하는 바람에 레드와인과 화이트 
    와인 표시가 누락되었습니다. 김 팀장은 다시 혼공머신을 불렀습니다.

    캔에 인쇄된 알코올 도수, 당도, pH 값으로 와인 종류를 구별할 수 있는 방법이있을까
    글쎼요 해봐야죠 일단 훈련데이터를 얻으려면 수천개의 캔을 뜯어야 할지도 몰라요
    품질확인용으로 뜬은 캔이 있으니 걱정 말게 필요한 데이터는 충분할 거네
    알겠습니다. 작업해보고 진전이 있으면 다시 말씀드릴꼐요
    이사님꼐 직접 보고해야 하니 조금이라도 진전이 있으면 바로 말해줘
    
    김팀장의 당부를 듣고 혼공머신은 일단 알코올 도수, 당도, pH 값에 로지스틱 회귀 모델을 적용할 계획을
    세웁니다.

    로지스틱 회귀로 와인 분류하기
    혼공머신은 의외로 문제를 쉽게 풀수 있을것 같았습니다. 품질관리 팀에서 6497개의 와인 샘플 데이터를 보냈습니다.
    이 데이터 셋을 불러와 보죠. 4장에서처럼 판다스를 사용해 인터넷에서 직접 불러오겠습니다.
'''

import pandas as pd
wine = pd.read_csv('https://bit.ly/wine-date')

# print(wine.head())

'''
    처음 3개의 열은 각각 알코올 도수,당도,pH값을 나타냅니다. 네번쨰 열은 타깃값으로 0이면 레드와인, 1이면 화이트
    와인이라고 하네요 레드 와인과 화이트와인을 구분하는 이진 분류 문제이고, 화이트 와인 양성 클래스입니다.
    즉 전체 와인 데이터에서 화이트 와인을 골라내는 문제군요.

    로지스틱 회귀 모델을 바로 훈련하기 전에 판다스 데이터프레임의 유용한 메서드 2개를 먼저 알아보겠습니다.
    먼저 info()메서드입니다. 이 메서드는 데이터프레임의 각 열의 데이터 타입과 누락된 데이터가
    있는지 확인하는데 유용합니다.
'''

# print(wine.info())

'''
    출력결과를 보면 총 6497개의 샘플이 있고 4개의 열은 모두 실솟값입니다. Non-NUll Count가 모두 6497이므로
    누락된 값은 없는것 같습니다.
'''

'''
    누락된 값이 있다면 그 데이터를 버리거나 평균값으로 채운 후 사용할 수 있습니다. 어떤 방식이 최선인지는 미리
    알기 어렵습니다. 두 가지 모두 시도해보세요. 여기에서도 항상 훈련 세트의 통계 값으로 테스트 변환하다는 것을
    잊지 마세요. 즉 훈련 세트의 평균값으로 테스트 세트의 누락된 값을 채워야 합니다.

    다음은 알아볼 메서드는 describe()입니다. 이 메서드는 열에 대한 간략한 통계를 출력해 줍니다.
    최소, 최대, 평균값등을 볼수 있습니다.
'''

print(wine.describe())

'''
    평균 표준편차 최소 최대 값을 볼 수 있습니다. 
    시분위수는 데이터를 순서대로 4등분 한 값입니다. 예를 들어 2사분위수(중간값)는 데이터를 일렬로 늘어놓을떄
    정중앙의 값입니다. 만약 데이터 개수가 짝수개라 중앙값을 선택할 수 없다면 가운데 2개 값의 평균을 사용합니다.

    여기서 알 수 있는 것이 알코올 도수와 당도 pH 값의 스케일이 다르다는 것입니다. 이전에 했던 것처럼 사이킷런의
    StandardScaler 클래스를 사용해 특성을 표준해해야겠군요. 그 전에 먼저 판다스 데이터 프레임을
    넘파이 배열로 바꾸고 훈련 세트와 테스트 세트로 나누겠습니다.
'''

data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()

'''
    wine 데이터프레임에서 처음 3개의 열을 넘파이 배열로 바꿔서 data 배열에 저장하고 마지막 class 열을 넘파이
    배열로 바꿔서 target 배열에 저장했습니다. 이제 훈련세트와 테스트 세트로 나누어보죠
'''

from sklearn.model_selection import train_test_split
train_input,test_input,train_target,test_target = train_test_split(data,target,test_size=0.2,random_state=42)

'''
    train_test_split()함수는 설정값을 지정하지 않으면 25%를 테스트 세트로 지정합니다.
    샘플 개수가 충분히 많으므로 20%정도만 테스트 세트로 나눴습니다. 코드의 test_size = 0.2가 이런 의미입니다.
    만들어진 훈련세트와 테스트 세트의 크기를 확인해보죠
'''

print(train_input.shape,test_input.shape)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled,train_target)
print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled,test_target))

'''
    음 점수가 높지 않군요 생각보다 화이트 와인을 골라내는게 어렵나 봅니다. 훈련세트와 테스트 세트의 점수가
    모두 낮으니 모델이 다소 과소적합된 것 같습니다. 이 문제를 해결하기 위해 규제 매개변수 C의 값을 바꿔 볼까요?
    아니면 solver매개변수에서 다른 알고리즘을 선택할 수도 있습니다. 또는 다항 특성을 만들어 추가 할 수도 있겠네요

    완벽하지 않지만, 조금이라도 진전이 있으니 김 팀장에게 보고해야겠군요.
'''

# 설명하기 쉬운 모델과 어려운 모델
'''
    혼공 머신은 김팀장과 함께 이사님에게 제출할 보고서를 만들려고 합니다. 이 모델을 설명하기 위해
    로지스틱 회귀가 학습한 계수와 절편을 출력해보죠.
'''

print(lr.coef_,lr.intercept_)

'''
    보고서
    이 모델은 알코올 도수 값에 0.51270274를 곱하고, 당도에 1.6733911을 곱하고, pH값에 -0.68767781을 곱한다음
    모두 더합니다. 마지막으로 1.81777902를 더합니다. 이 값이 0보다 크면 화이트 와인 작으면 레드 와인 입니다.
    현재 약 77% 정확히 와인으로 분류 했습니다.

    여러분은 이 로지스틱 회귀 모델을 잘 이해할 수 있나요? 사실 우리는 이 모델이 왜저런 계수 값을 학습했는지 정확히
    이해하기 어렵습니다. 그저 추측할 뿐이죠.

    아마도 알코올 도수와 당도가 높을수록 화이트 와인일 가능성이 높고, pH가 높을수록 레드 와인일 가능성이 높은 것
    같습니다. 하지만 정확히 이 숫자가 어떤 의미인지 설명하긴 어렵습니다. 더군다나 다항 특성을 추가 한다면
    설명하기가 더 어려울 것입니다. 대부분 머신러닝 모델은 이렇게 학습의 결과를 설명하기 어렵습니다
    
    혼공머신이 보고서를 상신했지만, 이사님은 이 보고서를 이해 할 수 없었습니다. 그런데 어려운 설명은 종종 
    엔지니어를 신뢰하지 않는 결과로 이어지는 불상사가 생깁니다!

    조금더 쉬운방법이 없나요 보고서를 보고 내가 이일을 계속 진행해야 할지 말지 도대체 결정 할 수가 없군요
    이렇게 순서도처럼 설명해서 다시 가져오세요

    이사님은 직접 화이트 보드에 순서도를 그리며 이렇게 모델을 만들어야 한다고 일장 연설을 합니다.
    이런 정말 큰일이군요 이렇게 쉬운 방법으로 설명할 수 있는 모델이 있을까요? 어쩌면 홍선배는 알고 있을지
    모릅니다.
'''
# 결정트리
'''
    홍 선태는 혼공 머신에게 결정트리 모델이 이유를 설명하기 쉽다라고 알려 주었습니다. 생각해보니 언뜻 책에서
    본 것도 같네요 결정 트리 모델은 스무고개와 같습니다. 다음의 오른쪽 그림처럼 질문을 하나씩 던져서
    정답을 맞춰가는 거죠

    데이터를 잘 나눌 수 있는 질문을 찾는 다면 계속 질문을 추가해서 분류 정확도를 높일 수 있습니다.
    이미 예상 했겠지만 사이킷런이 결정 트리 알고리즘을 제공합니다.
    사이킷런의 DecisionTreeClassifier 클래스를 사용해 결정 트리 모델을 훈련해보죠. 새로운 클래스이지만
    사용번은 이전과 동일합니다. fit()메서드를 호출해서 모델을 훈련한 다음 score()메서드로 정확도를
    평가해 보겠습니다.

    결정트리 모델을 만들떄 왜 random_state를 지정하나요?
    사이킷런의 결정 트리 알고리즘은 노드에서 최적의 분할을 찾기 전에 특성의 순서를 섞습니다. 따라서 약간의
    무작위성이 주입되는데 실행 할떄마다 점수가 조금씩 달라질 수 있기 때문입니다. 여기에서는 독자들이
    실습한 결과와 책의 내용이 같도록 유지하기 위해 random_state를 지정하지만 실전에서는 필요하지 않습니다.
'''

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)

dt.fit(train_scaled,train_target)
print(dt.score(train_scaled,train_target))
print(dt.score(test_scaled,test_target))

if dt.predict([[10,10,10,]]) == 1:
    print("화이트와인")
else:
    print('레드와인')
print(dt.predict([[20,20,20]]))

'''
    와 훈련세트에 대한 점수가 엄청 높군요 거의 모두 맞춘 것 같습니다. 테스트 세트의 성능은 그에 비해
    조금 낮습니다. 과대적합된 모델이라고 볼 수 있겠네요. 그런데 이 모델을 그림으로 어떻게 표현할 수 있을까요?
    친절하게도 사이킷런은 plot_tree() 함수를 사용해 결정트리를 이해하기 쉬운 트리 그림을 출력해 줍니다.
    위에서 만든 결정 트리 모델 객체를 plot_tree() 함수에 전달해서 어떤트리가 만들어졌는지 그려보죠
'''

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
# plt.figure(figsize=(10,7))
# plot_tree(dt)
# plt.show()

'''
    엄청난 트리가 만들어 졌습니다. 수양버들 나뭇잎처럼 늘어졌군요. 진짜 나무는 밑에서부터 하늘위로 자라지만
    결정 트리는 위에서부터 아래로 거꾸로 자라납니다. 맨 위의 노드를 루트 노트라고 부르고 맨 아래 끝에
    달린 노드를 리프 노드라고 합니다.
'''
# 노드 설명
'''
    노드는 결정 트리를 구성하는 핵심 요소입니다. 노드는 훈련 데이터의 특성에 대한 테스트를 표현합니다.
    예를 들어 현재 샘플의 당도가 -0.239보다 작거나 같은지 테스트합니다. 가지는 테스트의 결과를 나타내며 
    일반적으로 하나의 노드는 2개의 가지를 가집니다.
'''

'''
    너무 복잡하니 plot_tree()함수에서 트리의 깊이를 제한해서 출력해보죠 max_depth매개변수를 1로 주면
    루트노트를 제외하고 하나의 노드를 더 확장하여 그립니다. 또 filled 매개변수에서 클래스에
    맞게 노드의 색을 색을 칠할 수 있습니다. feature_names 매개변수에는 특성의 이름을 전달 할 수 있습니다.
    이렇게 하면 노드가 어떤 특성으로 나뉘는지 좀더 잘 이해할 수 있죠. 한번 이렇게 그려보겠습니다
'''

# plt.figure(figsize=(10,7))
# plot_tree(dt,max_depth=1,filled=True,feature_names=['alcohol','sugar','pH'])
# plt.show()

# 테스트 조건(sugar), 불순도(gini), 총 샘플 수 (samples), 클래스별 샘플수(values)

'''
    이제 하나씩 살펴볼까요? 루트 노드는 당도가 -0.239이하인지 질문을 합니다. 만약 어떤 샘플의 당고가 -0.239와
    같거나 작으면 왼쪽 가지로 갑니다. 그렇지 않으면 오른쪽 가지로 이동합니다.
    즉 왼쪽이 Yes, 오른쪽이 No입니다. 루트 노드의 총 샘플 수는 5197개 입니다. 
    이중에서 음성클래스(래드와인) 1258개이고, 양성클래스(화이트와인)는 3939개입니다. 이 값이 value에 나타납니다.

    이어서 왼쪽 노드를 살펴 보겠습니다. 이 노드는 당도가 더 낮은지를 물어보네요. 당도가 -0.802와 같거나 낮다면
    다시 왼쪽 가지로, 그렇치 않으면 오른쪽 가지로 이동합니다. 이 노드에서 음성클래스와 양성 클래스의 샘플 개수는
    1177개와 1745개 입니다. 루트 노드보다 양성 클래스, 즉 화이트 와인의 비율이 크게 줄어들었습니다.
    그 이유는 오른쪽 노드를 보면 알 수 있습니다.

    오른쪽 노드는 음성클래스 81개 이고 양성클래스가 2194개로 대부분의 화이트 와인 샘플이 이노드로 이동했습니다.
    노드의 바탕 색깔을 유심히 살펴보세요. 루트 노드보다 이 노드가 진하고, 왼쪽 노드는 더 연해지지 않나요?
    plot_tree()함수에서 filled = True 로 지정하면 클래스마다 색깔을 부여하고, 어떤 클래스의 비율이 높아지면
    점점 진한 색으로 표시합니다. 아주 직관적이네요.

    결정트리에서 예측하는 방법은 간단합니다. 리프노드에서 가장 많은 클래스가 예측 클래스가 됩니다.
    앞에서 보았던 k-최근접 이웃과 비슷하네요. 만약 이 결정 트리의 성장을 여기서 멈춘다면 왼쪽 노드에 도달한
    샘플과 오른쪽 노드에 도달한 샘플은 모두 양성 클래스로 예측 됩니다. 두 노드 모두 양성 클래스의 개수가 
    많기 떄문이죠

    만약 결정 트리 회귀 문제를 적용하면 리프 노드에 도달한 샘플의 타깃을 평균하여 예측값으로 사용합니다.
    사이킷런의 결정 트리 회귀 모델은 DecisionTreeRegressor입니다.

    그런데 노드 상자안에 gini라는 것이 있네요 이것이 무엇인지 좀 더 자세히 알아 보겠습니다.
'''
# 불순도
'''
    gini는 지니 불순도를 의미합니다. DecisionTreeClassifier클래스의 criterion 매개변수의 기본값이 gini입니다
    criterion 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정하는 것입니다.
    앞의 그린 트리에서 루트 노드는 어떻게 당도 -0.239를 기준으로 왼쪽과 오른쪽 노드로 나누었을까요?
    바로 criterion 매개변수에 지정한 지니 불순도를 사용합니다. 그럼 지니 불순도를 어떻게 계산하는지 알아보죠
    아주 간단합니다.

    지니 불순도는 클래스의 비율을 제곱해서 더한 다음 1에서 뺴면 됩니다.
    지니 불순도 = 1-(음성 클래스 비율제곱+양성 클래스비율제곱)

    이게 끝입니다. 다중 클래스의 문제라면 클래스가 더 많겠지만 계산하는 방법은 동일 합니다. 그럼 이전 트리
    그림에 있던 루트 노드의 지니 불순도를 계산해 봅시다. 루트 노드는 총 5197개의 샘플이 있고 그중에 1258개가
    음성 클래스 3939개가 양성클래스 있습니다.

    1-((1258/5197))

    졀정 트리 모델은 부모노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장 시킵니다. 부모 노드와 자식노드의
    불순도 차이가 가능한 크도록 트리를 성장시킵니다. 부모 노드와 자식 노드의 불순도 차이를 계산하는 방법을
    알아보죠. 먼저 자식 노드의 불순도를 샘플 개수에 비례하여 모두 더합니다. 그 다음 부모 노드의 불순도에서 빼면
    됩니다.
    예를들어 앞의 트리 그림에서 루트 노드를 부모 노드라 하면 왼쪽 노드와 오른쪽 노드가 자식 노드가 됩니다.
    왼쪽노드로는 2922개의 샘플이 이동했고, 오른쪽 노드로는 2275개의 샘플이 이동했습니다. 그럼 불순도의 차이는
    다음과 같이 계산합니다.
    부모의 불순도 - (왼쪽 노드 샘플수/부모의 샘플수)*왼쪽 노드 불순도 - (오른쪽노드 샘플수/보모의 샘플수)* 오른쪽
    노드 불순도 = 0.367-(2922/5197) * 0.481 - (2275/5197) * 0.069 = 0.066

    이런 부모와 자식 노드 사이의 불순도 차이를 정보 이득이라고 부릅니다. 좋습니다. 이제 결정 트리의 노드를 어떻게
    나누는지 이해했습니다. 이 알고리즘은 이득이 최대가 되도록 데이터를 나누는군요 이떄 지니 불순도를 기준으로
    사용합니다.
'''

#가지치기
'''
    열매를 잘 맺기 위해 과수원에서 가지치기를 하는 것처럼 결정 트리도 가지치기를 해야 합니다. 그렇치 않으면
    무작정 끝까지 자라나는 트리가 만들어지거든요. 훈련세트에는 아주 잘 맞겠지만 테스트 세트에서 점수는 
    그에 못 미칠 것입니다. 이를 두고 일반화가 잘 안될것 같다고 말합니다.
'''

dt = DecisionTreeClassifier(max_depth=3,random_state=42)
dt.fit(train_scaled,train_target)
print(dt.score(train_scaled,train_target))
print(dt.score(test_scaled,test_target))

# plt.figure(figsize=(10,7))
# plot_tree(dt,filled=True,feature_names=['alcolhol','sugar','pH'])
# plt.show()

'''
    훨씬 보기 좋군요. 그래프를 따라가면서 샘플이 어떻게 나눠지는지 확인할 수 있습니다. 루트 노드 다음에 있는
    깊이 1의 노드는 모두 당도를 기준으로 훈련세트를 나눕니다. 하지만 깊이 2의 노드는 맨 왼쪽 노드만
    당도를 기준으로 나누고 왼쪽에서 두번쨰 노드는 알코올 도수를 기준으로 나눕니다. 오른쪽 두 노드는 pH를 사용
    하네요

    깊이 3에 있는 노드가 최종노드인 리프노드입니다. 왼쪽에서 세번쨰에 있는 노드만 음성 클래스가 더 많습니다.
    이 노드에 도착해야만 레드와인으로 예측합니다. 그럼 루트 노드부터 이 노드까지 도달하려면 당도가 -0.239보다
    작고 또 -0.802보다도 작아야 합니다. 그리고 알코올 도수는 0.454보다 작아야 합니다. 그럼 세번쨰 노드에
    도달하네요. 즉 당도가 -0.802와 같거나 작은 와인중에 알코올 도수가 0.454와 같거나 작은것이
    레드와인입니다.

    그런데 -0.802라는 음수로 된 당도를 이사님꼐 어떻게 설명해야 할까요? 잠깐만요. 뭔가 이상하군요. 
    앞서 불순도를 기준으로 샘플을 나눈다고 했습니다. 불순도는 클래스별 비율을 가지고 계산했죠.
    샘플을 어떤 클래스 비율로 나누는지 계산할때 특성값의 스케일이 계산에 영향을 미칠까요?
    아니요. 특성값의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않습니다.
    따라서 표준화 전처리를 할 필요가 없습니다. 이것이 결정트리 알고리즘의 또다른 장점중 하나입니다.
'''

dt = DecisionTreeClassifier(max_depth=3,random_state=42)
dt.fit(train_input,train_target)
print(dt.score(train_input,train_target))
print(dt.score(test_input,test_target))

# plt.figure(figsize=(10,7))
# plot_tree(dt,filled=True,feature_names=['alcolhol','sugar','pH'])
# plt.show()

'''
    결과를 보면 같은 트리지만 특성값을 표준점수로 바꾸지 않은 터라 이해하기가 훨씬 쉽습니다. 
    당도가 1.625와 같거나 작은 와인중에 알코올 도수가 11,025와 같거나 작은 것이 레드 와인이군요
    그외에는 모두 화이트 와인으로 예측했습니다.

    마지막으로 결정트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산해 준니다. 이트리의 루트노드와
    깊이 1에서 당도를 사용했기 때문에 아마도 당도가 가장 유용한 특성중 하나일것 같습니다. 특성 중요도는 결정트리
    모델의 features_importances_속성에 저장되어 있습니다. 이 값을 출력해보죠
'''

print(dt.feature_importances_)

'''
    역시 두번쨰 특성이 당도가 0.87정도로 특성 중요도가 가장 높네요. 그다음 알코올 도수 , pH순입니다.
    이 값을 모두 더하면 1이 됩니다. 특성 중요도는 각 노드이 정보 이득과 전체 샘플에 대한 비율을
    곱한 후 특성별로 더하여 계산됩니다. 특성 중요도를 활용하면 결정트리 모델을 특성 선택에 활용할 수 있습니다.
    이것이 결정 트리 알고리즘의 또 다른 장점 중 하나입니다.

    이 모델은 비록 테스트 세트의 성능이 아주 높지 않아 많은 화이트 와인을 완벽하게 골라 내지는 못했습니다.
    이사님에게 보고하게는 아주 좋은 모델입니다.
'''

# 이해하기 쉬운 결정 트리 모델
'''
    한빛 마켓에서 가을 신상품으로 준비한 캔 와인은 실수로 와인의 종류가 캔에 인쇄되지 않았습니다.
    혼공 머신은 알코올 도수, 당도, pH 데이터를 기준으로 화이트 와인을 골라내는 이진 분류 로지스틱 회귀
    모델을 훈련했습니다. 하지만 이사님은 도통 이해할수 없다고 하네요

    그담에 혼공머신은 결정 트리를 사용해 레드와인과 화이트 라인을 분류하는 문제를 풀었습니다.
'''